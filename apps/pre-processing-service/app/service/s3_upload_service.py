import time
import asyncio
import aiohttp
from typing import List, Dict
from loguru import logger
from app.errors.CustomException import InvalidItemDataException
from app.model.schemas import RequestS3Upload
from app.utils.s3_upload_util import S3UploadUtil
from app.utils.response import Response


class S3UploadService:
    """6ë‹¨ê³„: í¬ë¡¤ë§ëœ ìƒí’ˆ ì´ë¯¸ì§€ë“¤ê³¼ ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œí•˜ëŠ” ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.s3_util = S3UploadUtil()

    async def upload_crawled_products_to_s3(self, request: RequestS3Upload) -> dict:
        """
        í¬ë¡¤ë§ëœ ìƒí’ˆë“¤ì˜ ì´ë¯¸ì§€ì™€ ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œí•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (6ë‹¨ê³„)
        """
        keyword = request.keyword  # í‚¤ì›Œë“œ ì¶”ê°€
        crawled_products = request.crawled_products
        base_folder = (
                request.base_folder or "product"
        )  # ğŸ”¸ ê¸°ë³¸ê°’ ë³€ê²½: product-images â†’ product

        logger.info(
            f"S3 ì—…ë¡œë“œ ì„œë¹„ìŠ¤ ì‹œì‘: keyword='{keyword}', {len(crawled_products)}ê°œ ìƒí’ˆ"
        )

        upload_results = []
        total_success_images = 0
        total_fail_images = 0

        try:
            # HTTP ì„¸ì…˜ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            async with aiohttp.ClientSession() as session:

                # ê° ìƒí’ˆë³„ë¡œ ìˆœì°¨ ì—…ë¡œë“œ
                for product_info in crawled_products:
                    product_index = product_info.get("index", 0)
                    product_detail = product_info.get("product_detail")

                    logger.info(
                        f"ìƒí’ˆ {product_index}/{len(crawled_products)} S3 ì—…ë¡œë“œ ì‹œì‘"
                    )

                    # í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ìƒí’ˆì€ ìŠ¤í‚µ
                    if not product_detail or product_info.get("status") != "success":
                        logger.warning(
                            f"ìƒí’ˆ {product_index}: í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ ì¸í•œ ì—…ë¡œë“œ ìŠ¤í‚µ"
                        )
                        upload_results.append(
                            {
                                "product_index": product_index,
                                "product_title": "Unknown",
                                "status": "skipped",
                                "folder_s3_url": None,
                                "uploaded_images": [],
                                "success_count": 0,
                                "fail_count": 0,
                            }
                        )
                        continue

                    try:
                        # ìƒí’ˆ ì´ë¯¸ì§€ + ë°ì´í„° ì—…ë¡œë“œ (í‚¤ì›Œë“œ ì „ë‹¬ ì¶”ê°€!)
                        # ğŸ”¸ ì „ì²´ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì „ë‹¬ (product_detailì´ ì•„ë‹Œ product_info ì „ì²´)
                        upload_result = await self.s3_util.upload_single_product_images(
                            session,
                            product_info,
                            product_index,
                            keyword,
                            base_folder,  # product_detail â†’ product_info
                        )

                        upload_results.append(upload_result)
                        total_success_images += upload_result["success_count"]
                        total_fail_images += upload_result["fail_count"]

                        logger.success(
                            f"ìƒí’ˆ {product_index} S3 ì—…ë¡œë“œ ì™„ë£Œ: ì„±ê³µ {upload_result['success_count']}ê°œ, "
                            f"ì‹¤íŒ¨ {upload_result['fail_count']}ê°œ"
                        )

                    except Exception as e:
                        logger.error(f"ìƒí’ˆ {product_index} S3 ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                        upload_results.append(
                            {
                                "product_index": product_index,
                                "product_title": product_detail.get("title", "Unknown"),
                                "status": "error",
                                "folder_s3_url": None,
                                "uploaded_images": [],
                                "success_count": 0,
                                "fail_count": 0,
                            }
                        )

                    # ìƒí’ˆê°„ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    if product_index < len(crawled_products):
                        await asyncio.sleep(1)

            # ğŸ†• ì„ì‹œ: ì½˜í…ì¸  ìƒì„±ìš© ë‹¨ì¼ ìƒí’ˆ ì„ íƒ ë¡œì§
            selected_product_for_content = self._select_single_product_for_content(
                crawled_products, upload_results
            )

            logger.success(
                f"S3 ì—…ë¡œë“œ ì„œë¹„ìŠ¤ ì™„ë£Œ: ì´ ì„±ê³µ ì´ë¯¸ì§€ {total_success_images}ê°œ, ì´ ì‹¤íŒ¨ ì´ë¯¸ì§€ {total_fail_images}ê°œ"
            )

            # ê¸°ì¡´ ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            data = {
                "upload_results": upload_results,
                "summary": {
                    "total_products": len(crawled_products),
                    "total_success_images": total_success_images,
                    "total_fail_images": total_fail_images,
                },
                "uploaded_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                # ğŸ†• ì„ì‹œ: ì½˜í…ì¸  ìƒì„±ìš© ë‹¨ì¼ ìƒí’ˆë§Œ ì¶”ê°€ (ë‚˜ì¤‘ì— ì‚­ì œ ì˜ˆì •)
                "selected_product_for_content": selected_product_for_content,
            }

            message = f"S3 ì—…ë¡œë“œ ì™„ë£Œ: {total_success_images}ê°œ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„±ê³µ, ìƒí’ˆ ë°ì´í„° JSON íŒŒì¼ í¬í•¨"
            return Response.ok(data, message)

        except Exception as e:
            logger.error(f"S3 ì—…ë¡œë“œ ì„œë¹„ìŠ¤ ì „ì²´ ì˜¤ë¥˜: {e}")
            raise InvalidItemDataException()

    def _select_single_product_for_content(
            self, crawled_products: List[Dict], upload_results: List[Dict]
    ) -> Dict:
        """
        ğŸ†• ì„ì‹œ: ì½˜í…ì¸  ìƒì„±ì„ ìœ„í•œ ë‹¨ì¼ ìƒí’ˆ ì„ íƒ ë¡œì§
        ìš°ì„ ìˆœìœ„: 1) S3 ì—…ë¡œë“œ ì„±ê³µí•œ ìƒí’ˆ ì¤‘ ì´ë¯¸ì§€ ê°œìˆ˜ê°€ ë§ì€ ê²ƒ
                 2) ì—†ë‹¤ë©´ í¬ë¡¤ë§ ì„±ê³µí•œ ì²« ë²ˆì§¸ ìƒí’ˆ
        """
        try:
            # 1ìˆœìœ„: S3 ì—…ë¡œë“œ ì„±ê³µí•˜ê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒí’ˆë“¤
            successful_uploads = [
                result for result in upload_results
                if result.get("status") == "completed" and result.get("success_count", 0) > 0
            ]

            if successful_uploads:
                # ì´ë¯¸ì§€ ê°œìˆ˜ê°€ ê°€ì¥ ë§ì€ ìƒí’ˆ ì„ íƒ
                best_upload = max(successful_uploads, key=lambda x: x.get("success_count", 0))
                selected_index = best_upload["product_index"]

                # ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ í•´ë‹¹ ìƒí’ˆ ì°¾ê¸°
                for product_info in crawled_products:
                    if product_info.get("index") == selected_index:
                        logger.info(
                            f"ì½˜í…ì¸  ìƒì„±ìš© ìƒí’ˆ ì„ íƒ: index={selected_index}, "
                            f"title='{product_info.get('product_detail', {}).get('title', 'Unknown')[:30]}', "
                            f"images={best_upload.get('success_count', 0)}ê°œ"
                        )
                        return {
                            "selection_reason": "s3_upload_success_with_most_images",
                            "product_info": product_info,
                            "s3_upload_info": best_upload,
                        }

            # 2ìˆœìœ„: í¬ë¡¤ë§ ì„±ê³µí•œ ì²« ë²ˆì§¸ ìƒí’ˆ (S3 ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„)
            for product_info in crawled_products:
                if (product_info.get("status") == "success" and
                        product_info.get("product_detail")):

                    # í•´ë‹¹ ìƒí’ˆì˜ S3 ì—…ë¡œë“œ ì •ë³´ ì°¾ê¸°
                    upload_info = None
                    for result in upload_results:
                        if result.get("product_index") == product_info.get("index"):
                            upload_info = result
                            break

                    logger.info(
                        f"ì½˜í…ì¸  ìƒì„±ìš© ìƒí’ˆ ì„ íƒ (fallback): index={product_info.get('index')}, "
                        f"title='{product_info.get('product_detail', {}).get('title', 'Unknown')[:30]}'"
                    )
                    return {
                        "selection_reason": "first_crawl_success",
                        "product_info": product_info,
                        "s3_upload_info": upload_info,
                    }

            # 3ìˆœìœ„: ì•„ë¬´ê±°ë‚˜ (ëª¨ë“  ìƒí’ˆì´ ì‹¤íŒ¨í•œ ê²½ìš°)
            if crawled_products:
                logger.warning("ëª¨ë“  ìƒí’ˆì´ í¬ë¡¤ë§ ì‹¤íŒ¨ - ì²« ë²ˆì§¸ ìƒí’ˆìœ¼ë¡œ fallback")
                return {
                    "selection_reason": "fallback_first_product",
                    "product_info": crawled_products[0],
                    "s3_upload_info": upload_results[0] if upload_results else None,
                }

            logger.error("ì„ íƒí•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                "selection_reason": "no_products_available",
                "product_info": None,
                "s3_upload_info": None,
            }

        except Exception as e:
            logger.error(f"ë‹¨ì¼ ìƒí’ˆ ì„ íƒ ì˜¤ë¥˜: {e}")
            return {
                "selection_reason": "selection_error",
                "product_info": crawled_products[0] if crawled_products else None,
                "s3_upload_info": upload_results[0] if upload_results else None,
                "error": str(e),
            }